{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama Anggota:\n",
        " 1. Daniel\n",
        " 2. Bella Anggraini\n",
        " 3. Rafly Surya Wibowo\n",
        " 4. Halimah Salsabila\n",
        " 5. Santa Genecia\n",
        " 6. Nurul Muflikha\n"
      ],
      "metadata": {
        "id": "AJmCRkHPKUnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapan"
      ],
      "metadata": {
        "id": "rM-huOKr6MoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat file path / directory di google drive"
      ],
      "metadata": {
        "id": "GFXXnrJOGkRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csh0B4nyEZ0c",
        "outputId": "324f84f4-bde9-40ca-b095-e32cef1b63fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Assignment 5 - Introduction to Artificial Intelligence/Artificial Intelligence/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXAqSu1JFCG6",
        "outputId": "189dfeb7-c6f2-4944-db07-c40aae544406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Assignment 5 - Introduction to Artificial Intelligence/Artificial Intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er2nwjoDGcfK",
        "outputId": "e9bfb21f-1e67-4198-d359-f97fd2a69123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Assignment 5 - Introduction to Artificial Intelligence/Artificial Intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "GeJ2YDTU9P0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "## Tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "## Wordnet\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "## Stop Words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "## Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "## Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "## Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-DbH_9P9WGc",
        "outputId": "aea2b602-3153-4e44-b0a6-2ef6a2c356e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "Lc3I7fSN9a83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer() ## Create object for lemmatizer"
      ],
      "metadata": {
        "id": "QVUceTYx9d71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat fungsi untuk mencari kata dasar dari sebuah kata berdasarkan WordNet"
      ],
      "metadata": {
        "id": "SbcK9pKw7-iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CountVectorizer"
      ],
      "metadata": {
        "id": "k1Uf-bQ86H4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "I9lcvZRz6U4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat fungsi untuk menghitung jumlah vektor"
      ],
      "metadata": {
        "id": "xMv3ricC7734"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Menghitung vektor Bag-of-Words tiap topik"
      ],
      "metadata": {
        "id": "Ao_fpJCY6t9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topik Olahraga"
      ],
      "metadata": {
        "id": "1vYuJhfK64Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sport = \"Former Barcelona striker Gary Lineker has always been an outspoken champion of Messi, to the point that some critics have accused him of bias in his coverage as a presenter for BBC Sport. But his post-World Cup final tribute to the Argentina captain on social media was perfect. It was impossible not to feel emotional at the sight of Messi lifting the one trophy that had eluded him for so long in Qatar. He had fallen short with Argentina at the final stage in 2014, and had struggled with the burden of huge expectations on the international stage ever since his debut as a long-haired teenager nine years earlier.\"\n",
        "\n",
        "\n",
        "text_sport = text_sport.lower()\n",
        "text_sport = text_sport.strip()\n",
        "text_sport = re.sub(\"[^A-Za-z\\s']\",\" \", text_sport) #Menghilangkan yang bukan huruf\n",
        "tokens_sport = word_tokenize(text_sport)\n",
        "text_sport_lemma = [lemmatizer.lemmatize(word) for word in tokens_sport if word not in set(stopwords.words(\"english\"))]\n",
        "text_sport = ' '.join(text_sport_lemma)\n",
        "text_sport = cv.fit_transform([text_sport]).toarray()\n",
        "print(text_sport_lemma)\n",
        "print(len(text_sport[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCKV5uZS6xfI",
        "outputId": "2078297d-50fc-4a1d-d485-410bfb4639f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['former', 'barcelona', 'striker', 'gary', 'lineker', 'always', 'outspoken', 'champion', 'messi', 'point', 'critic', 'accused', 'bias', 'coverage', 'presenter', 'bbc', 'sport', 'post', 'world', 'cup', 'final', 'tribute', 'argentina', 'captain', 'social', 'medium', 'perfect', 'impossible', 'feel', 'emotional', 'sight', 'messi', 'lifting', 'one', 'trophy', 'eluded', 'long', 'qatar', 'fallen', 'short', 'argentina', 'final', 'stage', 'struggled', 'burden', 'huge', 'expectation', 'international', 'stage', 'ever', 'since', 'debut', 'long', 'haired', 'teenager', 'nine', 'year', 'earlier']\n",
            "53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa seteah melakukan teks preprocessing dapat diketahui bahwa artikel dari topik olahraga yang dipilih mempunyai vektor bag-of-words berjumlah 53 buah."
      ],
      "metadata": {
        "id": "ZcX1hYtG7jnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topik Kesehatan"
      ],
      "metadata": {
        "id": "BBIEgMYk67Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_medical = \"The cardiovascular system consists of the heart and its blood vessels. A wide array of problems can arise within the cardiovascular system, a few of which include endocarditis, rheumatic heart disease, and conduction system abnormalities. Cardiovascular disease, also known as heart disease, refers to the following 4 entities: coronary artery disease (CAD) which is also referred to as coronary heart disease (CHD), cerebrovascular disease, peripheral artery disease (PAD), and aortic atherosclerosis. CAD results from decreased myocardial perfusion that causes angina due to ischemia and can result in myocardial infarction (MI), and/or heart failure. It accounts for one-third to one-half of all cases of cardiovascular disease. Cerebrovascular disease is the entity associated with strokes, also termed cerebrovascular accidents, and transient ischemic attacks (TIAs).\"\n",
        "\n",
        "text_medical = text_medical.lower()\n",
        "text_medical = text_medical.strip()\n",
        "text_medical = re.sub(\"[^A-Za-z\\s']\",\" \", text_medical) #Menghilangkan yang bukan huruf\n",
        "tokens_medical = word_tokenize(text_medical)\n",
        "text_medical_lemma = [lemmatizer.lemmatize(word) for word in tokens_medical if word not in set(stopwords.words(\"english\"))]\n",
        "text_medical = ' '.join(text_medical_lemma)\n",
        "text_medical = cv.fit_transform([text_medical]).toarray()\n",
        "print(text_medical_lemma)\n",
        "print(len(text_medical[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hoaezin6zLT",
        "outputId": "3392785e-eded-4838-f8ff-add1935ca3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cardiovascular', 'system', 'consists', 'heart', 'blood', 'vessel', 'wide', 'array', 'problem', 'arise', 'within', 'cardiovascular', 'system', 'include', 'endocarditis', 'rheumatic', 'heart', 'disease', 'conduction', 'system', 'abnormality', 'cardiovascular', 'disease', 'also', 'known', 'heart', 'disease', 'refers', 'following', 'entity', 'coronary', 'artery', 'disease', 'cad', 'also', 'referred', 'coronary', 'heart', 'disease', 'chd', 'cerebrovascular', 'disease', 'peripheral', 'artery', 'disease', 'pad', 'aortic', 'atherosclerosis', 'cad', 'result', 'decreased', 'myocardial', 'perfusion', 'cause', 'angina', 'due', 'ischemia', 'result', 'myocardial', 'infarction', 'mi', 'heart', 'failure', 'account', 'one', 'third', 'one', 'half', 'case', 'cardiovascular', 'disease', 'cerebrovascular', 'disease', 'entity', 'associated', 'stroke', 'also', 'termed', 'cerebrovascular', 'accident', 'transient', 'ischemic', 'attack', 'tia']\n",
            "56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa seteah melakukan teks preprocessing dapat diketahui bahwa artikel dari topik kesehatan yang dipilih mempunyai vektor bag-of-words berjumlah 56 buah."
      ],
      "metadata": {
        "id": "Jk07RHrK7rUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topik Keuangan"
      ],
      "metadata": {
        "id": "_YYhx3pP69na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_finance = \"Earlier this week, President Joe Biden formally announced a significant new student loan forgiveness plan that could provide relief to millions of borrowers. The initiative, which has been in the works for months, represents Bidens second attempt to cancel student debt on a mass scale. The new loan forgiveness program, which hasnt been officially finalized or made available quite yet, will target specific groups of borrowers for relief. But when its officially launched — which could happen by this fall — many borrowers may not need to submit an application. Millions could receive student loan forgiveness automatically.\"\n",
        "\n",
        "text_finance = text_finance.lower()\n",
        "text_finance = text_finance.strip()\n",
        "text_finance = re.sub(\"[^A-Za-z\\s']\",\" \", text_finance) #Menghilangkan yang bukan huruf\n",
        "tokens_finance = word_tokenize(text_finance)\n",
        "text_finance_lemma = [lemmatizer.lemmatize(word) for word in tokens_finance if word not in set(stopwords.words(\"english\"))]\n",
        "text_finance = ' '.join(text_finance_lemma)\n",
        "text_finance = cv.fit_transform([text_finance]).toarray()\n",
        "print(text_finance_lemma)\n",
        "print(len(text_finance[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UsdM0If61CB",
        "outputId": "7863e3b6-6dca-4ae2-dda3-4c82fb26946e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['earlier', 'week', 'president', 'joe', 'biden', 'formally', 'announced', 'significant', 'new', 'student', 'loan', 'forgiveness', 'plan', 'could', 'provide', 'relief', 'million', 'borrower', 'initiative', 'work', 'month', 'represents', 'bidens', 'second', 'attempt', 'cancel', 'student', 'debt', 'mass', 'scale', 'new', 'loan', 'forgiveness', 'program', 'hasnt', 'officially', 'finalized', 'made', 'available', 'quite', 'yet', 'target', 'specific', 'group', 'borrower', 'relief', 'officially', 'launched', 'could', 'happen', 'fall', 'many', 'borrower', 'may', 'need', 'submit', 'application', 'million', 'could', 'receive', 'student', 'loan', 'forgiveness', 'automatically']\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa seteah melakukan teks preprocessing dapat diketahui bahwa artikel dari topik keuangan yang dipilih mempunyai vektor bag-of-words berjumlah 50 buah."
      ],
      "metadata": {
        "id": "Z5kaTLNP7wGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Klasifikasi teks ke dalam topik yang benar"
      ],
      "metadata": {
        "id": "87ZmGY0O9fuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three texts with labeled topic\n",
        "text_sport = \"Former Barcelona striker Gary Lineker has always been an outspoken champion of Messi, to the point that some critics have accused him of bias in his coverage as a presenter for BBC medical. But his post-World Cup final tribute to the Argentina captain on social media was perfect. It was impossible not to feel emotional at the sight of Messi lifting the one trophy that had eluded him for so long in Qatar. He had fallen short with Argentina at the final stage in 2014, and had struggled with the burden of huge expectations on the international stage ever since his debut as a long-haired teenager nine years earlier.\"\n",
        "text_medical = \"The cardiovascular system consists of the heart and its blood vessels. A wide array of problems can arise within the cardiovascular system, a few of which include endocarditis, rheumatic heart disease, and conduction system abnormalities. Cardiovascular disease, also known as heart disease, refers to the following 4 entities: coronary artery disease (CAD) which is also referred to as coronary heart disease (CHD), cerebrovascular disease, peripheral artery disease (PAD), and aortic atherosclerosis. CAD results from decreased myocardial perfusion that causes angina due to ischemia and can result in myocardial infarction (MI), and/or heart failure. It accounts for one-third to one-half of all cases of cardiovascular disease. Cerebrovascular disease is the entity associated with strokes, also termed cerebrovascular accidents, and transient ischemic attacks (TIAs).\"\n",
        "text_finance = \"Earlier this week, President Joe Biden formally announced a significant new student loan forgiveness plan that could provide relief to millions of borrowers. The initiative, which has been in the works for months, represents Bidens second attempt to cancel student debt on a mass scale. The new loan forgiveness program, which hasnt been officially finalized or made available quite yet, will target specific groups of borrowers for relief. But when its officially launched — which could happen by this fall — many borrowers may not need to submit an application. Millions could receive student loan forgiveness automatically.\"\n",
        "\n",
        "texts = [text_sport, text_medical, text_finance]\n",
        "bow_keys = []\n",
        "corpus_texts = []\n",
        "for text in texts:\n",
        "    text_lower = text.lower()\n",
        "    text_strip = text_lower.strip()\n",
        "    text_sub = re.sub(\"[^A-Za-z\\s']\",\" \", text_strip) #Menghilangkan yang bukan huruf\n",
        "    words = word_tokenize(text_sub)\n",
        "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    bow_keys += texts\n",
        "    text = ' '.join(texts)\n",
        "    corpus_texts.append(text)\n",
        "bow_keys = set(bow_keys)\n",
        "print(bow_keys) #### Cleaned Data\n",
        "print(corpus_texts) #### Cleaned Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSN_dk49hT3",
        "outputId": "4b691c9c-5f99-426b-821f-232c68c180f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yet', 'account', 'result', 'failure', 'second', 'struggled', 'mi', 'loan', 'may', 'cause', 'earlier', 'atherosclerosis', 'relief', 'medium', 'striker', 'million', 'hasnt', 'champion', 'chd', 'artery', 'joe', 'critic', 'trophy', 'qatar', 'long', 'referred', 'specific', 'known', 'third', 'aortic', 'announced', 'forgiveness', 'emotional', 'coronary', 'scale', 'bbc', 'decreased', 'submit', 'consists', 'attack', 'made', 'world', 'blood', 'student', 'wide', 'cancel', 'need', 'biden', 'entity', 'endocarditis', 'could', 'coverage', 'within', 'teenager', 'stage', 'presenter', 'short', 'peripheral', 'always', 'finalized', 'sight', 'one', 'haired', 'ever', 'termed', 'significant', 'vessel', 'week', 'cardiovascular', 'quite', 'accused', 'fallen', 'also', 'international', 'myocardial', 'perfusion', 'disease', 'former', 'conduction', 'messi', 'lineker', 'launched', 'impossible', 'bias', 'cad', 'pad', 'following', 'arise', 'stroke', 'program', 'officially', 'fall', 'automatically', 'group', 'point', 'since', 'due', 'burden', 'happen', 'ischemia', 'receive', 'medical', 'formally', 'expectation', 'system', 'gary', 'represents', 'president', 'month', 'bidens', 'lifting', 'borrower', 'ischemic', 'cerebrovascular', 'many', 'array', 'mass', 'huge', 'debt', 'transient', 'half', 'perfect', 'tia', 'include', 'nine', 'year', 'feel', 'plan', 'infarction', 'case', 'outspoken', 'captain', 'accident', 'heart', 'debut', 'target', 'initiative', 'available', 'barcelona', 'provide', 'eluded', 'attempt', 'tribute', 'rheumatic', 'angina', 'work', 'argentina', 'problem', 'refers', 'associated', 'application', 'new', 'post', 'social', 'cup', 'abnormality', 'final'}\n",
            "['former barcelona striker gary lineker always outspoken champion messi point critic accused bias coverage presenter bbc medical post world cup final tribute argentina captain social medium perfect impossible feel emotional sight messi lifting one trophy eluded long qatar fallen short argentina final stage struggled burden huge expectation international stage ever since debut long haired teenager nine year earlier', 'cardiovascular system consists heart blood vessel wide array problem arise within cardiovascular system include endocarditis rheumatic heart disease conduction system abnormality cardiovascular disease also known heart disease refers following entity coronary artery disease cad also referred coronary heart disease chd cerebrovascular disease peripheral artery disease pad aortic atherosclerosis cad result decreased myocardial perfusion cause angina due ischemia result myocardial infarction mi heart failure account one third one half case cardiovascular disease cerebrovascular disease entity associated stroke also termed cerebrovascular accident transient ischemic attack tia', 'earlier week president joe biden formally announced significant new student loan forgiveness plan could provide relief million borrower initiative work month represents bidens second attempt cancel student debt mass scale new loan forgiveness program hasnt officially finalized made available quite yet target specific group borrower relief officially launched could happen fall many borrower may need submit application million could receive student loan forgiveness automatically']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A new text to be classified based on on topic\n",
        "query_text = \"Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process. Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres.\"\n",
        "query_lower = query_text.lower()\n",
        "query_strip = query_lower.strip()\n",
        "query_sub = re.sub(\"[^A-Za-z\\s']\",\" \", query_strip) #Menghilangkan yang bukan huruf\n",
        "query_words = word_tokenize(query_sub)\n",
        "query_words_clean = [lemmatizer.lemmatize(word) for word in query_words if word not in set(stopwords.words('english'))]\n",
        "query_words_corpus = [word for word in query_words_clean if word in set(bow_keys)]\n",
        "query_text_corpus = ' '.join(query_words_corpus)\n",
        "corpus_texts.append(query_text_corpus)\n",
        "\n",
        "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
        "bow_vectors = cv.fit_transform(corpus_texts).toarray()\n",
        "print(bow_vectors)\n",
        "print(len(bow_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUFVG-9g9jC8",
        "outputId": "16071176-a8fb-4b7c-9bf4-bf32cd99baf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1\n",
            "  0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 2 0 0 0 0 1 1 0 1 0 0 0\n",
            "  0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 2 0 0 0 0 1 1 2 0 0 0 0 0 0 1 0 1 1 0 1 0\n",
            "  0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 2 1 0 1 0 0 0 0 1 0\n",
            "  0 0 0 1 1 0 0 0 0 0 1 1 0]\n",
            " [1 1 1 0 3 0 1 0 1 0 0 1 1 2 1 1 1 0 0 0 0 0 0 0 0 1 0 0 2 0 0 4 1 1 3 0\n",
            "  1 1 1 2 0 0 0 0 0 0 1 9 1 0 0 0 1 2 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            "  5 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 2 0 1 0 1\n",
            "  1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 2 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 3 0 0 1\n",
            "  1 1 1 0 0 1 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 3 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 3 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 3 1 0 0 1 0 0 1 1\n",
            "  0 0 0 0 0 1 0 0 0 1 0 1 0 0 3 0 1 1 1 1 0 0 0 0 2 1 0 1 2 0 2 0 0 0 0 0\n",
            "  0 1 0 0 0 1 0 1 1 0 1 1 0 0 2 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 3 1 0 1 0 0\n",
            "  0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the Bow vectors\n",
        "bow_texts_norm = []\n",
        "for bow in bow_vectors:\n",
        "    length = np.linalg.norm(bow)\n",
        "    if length != 0:\n",
        "        bow_norm = bow / length\n",
        "        bow_texts_norm.append(bow_norm)\n",
        "    else:\n",
        "        bow_texts_norm.append(bow)\n",
        "\n",
        "# Compute similarity using dot product\n",
        "bow_norm_query = bow_texts_norm[3]\n",
        "max_similarity = -1\n",
        "max_index = -1\n",
        "for idx, bow in enumerate(bow_texts_norm[:3]):\n",
        "    similarity = np.dot(bow, bow_norm_query)\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        max_index = idx\n",
        "\n",
        "# Find the highest similarity\n",
        "if max_index == 0:\n",
        "    print(\"The query text is classified as: Sport\")\n",
        "elif max_index == 1:\n",
        "    print(\"The query text is classified as: Medical\")\n",
        "elif max_index == 2:\n",
        "    print(\"The query text is classified as: Finance\")\n"
      ],
      "metadata": {
        "id": "WkC7IdYSN8qD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68eaf6b5-5108-44b8-d150-b1addf02af14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query text is classified as: Sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat hasil dari klasifikasi kemipiran teks berdasarkan nilai\n",
        "indeks kemiripan teks terhadap masing-masing artikel topik yang berbeda. Diketahui bahwa teks tersebut dapat diklasifikasikan menjadi topik olahraga dikarenakan kemiripan dalam kata-katanya yang sangat tinggi."
      ],
      "metadata": {
        "id": "JHycH_aZSQq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sumber artikel:\n",
        "1. Topik olahraga: https://www.goal.com/en/lists/lionel-messi-completed-football-argentina-world-cup-win-2023-ballon-dor/blt2ef8bfdaf8f38dbc#cs99fcb03b97fd88b8\n",
        "\n",
        "2. Topik kesehatan: https://www.ncbi.nlm.nih.gov/books/NBK535419/\n",
        "\n",
        "3. Topik keuangan: https://www.forbes.com/sites/adamminsky/2024/04/11/student-loan-forgiveness-application-for-bidens-new-plan-what-we-know/?sh=6941c743271b"
      ],
      "metadata": {
        "id": "tMtwIgRgWnk_"
      }
    }
  ]
}